For each model, we provide COMET and spBLEU scores for each language pair in flores101. Additionally, 
we include the average scores across 101 languages for the corresponding central language (summary_101.csv). 
To compare with the m2m100 model, which supports only 85 languages, 
we also provide the average scores across those 85 languages (summary_85.csv).
For the results of each language pair for M2M-100 and Lego-MT, please refer to the [Lego-MT](https://github.com/CONE-MT/Lego-MT).

Note: we followed the authors' recommended prompts and parameters as closely as possible when evaluating the models, but we cannot guarantee that we fully optimized each model's performance. If you have any questions regarding the model results, please feel free to contact us.